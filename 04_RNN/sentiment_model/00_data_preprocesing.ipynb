{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "    - Download data to the server\n",
    "    - Convert text to sequences.\n",
    "    - Configure sequences for a RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data to the server\n",
    "\n",
    "### Command line in the server\n",
    "    Path to data:\n",
    "        cd /home/ubuntu/data/training/text/sentiment\n",
    "    Download dataset: \n",
    "        wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    Uncompress it:\n",
    "        tar -zxvf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text to sequences\n",
    "    - List of all text files\n",
    "    - Read files into python\n",
    "    - Tokenize\n",
    "    - Create dictionaries to recode\n",
    "    - Recode tokens into ids and create sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and paths\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# GPU path\n",
    "#data_path='/home/ubuntu/data/training/text/sentiment/aclImdb/'\n",
    "\n",
    "data_path='../../data/aclImdb/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator of list of files in a folder and subfolders\n",
    "import os\n",
    "import shutil\n",
    "import fnmatch\n",
    "\n",
    "def gen_find(filepattern, toppath):\n",
    "    '''\n",
    "    Generator with a recursive list of files in the toppath that match filepattern \n",
    "    Inputs:\n",
    "        filepattern(str): Command stype pattern \n",
    "        toppath(str): Root path\n",
    "    '''\n",
    "    for path, dirlist, filelist in os.walk(toppath):\n",
    "        for name in fnmatch.filter(filelist, filepattern):\n",
    "            yield os.path.join(path, name)\n",
    "\n",
    "#Test\n",
    "print(next(gen_find(\"*.txt\", data_path+'train/pos/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(path):\n",
    "    sentences = []\n",
    "    sentences_list = gen_find(\"*.txt\", path)\n",
    "    for ff in sentences_list:\n",
    "        with open(ff, 'r', encoding='utf8') as f:\n",
    "            sentences.append(f.readline().strip())\n",
    "    return sentences        \n",
    "\n",
    "#Test\n",
    "print(read_sentences(data_path+'train/pos/')[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read_sentences(data_path+'train/neg/')[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    from nltk import word_tokenize\n",
    "    print( 'Tokenizing...',)\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        tokens += [word_tokenize(sentence)]\n",
    "    print('Done!')\n",
    "\n",
    "    return tokens\n",
    "\n",
    "print(tokenize(read_sentences(data_path+'train/pos/')[0:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_trn_pos = tokenize(read_sentences(data_path+'train/pos/'))\n",
    "sentences_trn_neg = tokenize(read_sentences(data_path+'train/neg/'))\n",
    "sentences_trn = sentences_trn_pos + sentences_trn_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the dictionary to conver words to numbers. Order it with most frequent words first\n",
    "def build_dict(sentences):\n",
    "#    from collections import OrderedDict\n",
    "\n",
    "    '''\n",
    "    Build dictionary of train words\n",
    "    Outputs: \n",
    "     - Dictionary of word --> word index\n",
    "     - Dictionary of word --> word count freq\n",
    "    '''\n",
    "    print( 'Building dictionary..',)\n",
    "    wordcount = dict()\n",
    "    #For each worn in each sentence, cummulate frequency\n",
    "    for ss in sentences:\n",
    "        for w in ss:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 1\n",
    "            else:\n",
    "                wordcount[w] += 1\n",
    "\n",
    "    counts = list(wordcount.values()) # List of frequencies\n",
    "    keys = list(wordcount) #List of words\n",
    "    \n",
    "    sorted_idx = reversed(np.argsort(counts))\n",
    "    \n",
    "    worddict = dict()\n",
    "    for idx, ss in enumerate(sorted_idx):\n",
    "        worddict[keys[ss]] = idx+2  # leave 0 and 1 (UNK)\n",
    "    print( np.sum(counts), ' total words ', len(keys), ' unique words')\n",
    "\n",
    "    return worddict, wordcount\n",
    "\n",
    "\n",
    "worddict, wordcount = build_dict(sentences_trn)\n",
    "\n",
    "print(worddict['the'], wordcount['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def generate_sequence(sentences, dictionary):\n",
    "    '''\n",
    "    Convert tokenized text in sequences of integers\n",
    "    '''\n",
    "    seqs = [None] * len(sentences)\n",
    "    for idx, ss in enumerate(sentences):\n",
    "        seqs[idx] = [dictionary[w] if w in dictionary else 1 for w in ss]\n",
    "\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test data\n",
    "\n",
    "#Read train sentences and generate target y\n",
    "train_x_pos = generate_sequence(sentences_trn_pos, worddict)\n",
    "train_x_neg = generate_sequence(sentences_trn_neg, worddict)\n",
    "X_train_full = train_x_pos + train_x_neg\n",
    "y_train_full = [1] * len(train_x_pos) + [0] * len(train_x_neg)\n",
    "\n",
    "print(X_train_full[0], y_train_full[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read test sentences and generate target y\n",
    "sentences_tst_pos = read_sentences(data_path+'test/pos/')\n",
    "sentences_tst_neg = read_sentences(data_path+'test/neg/')\n",
    "\n",
    "test_x_pos = generate_sequence(tokenize(sentences_tst_pos), worddict)\n",
    "test_x_neg = generate_sequence(tokenize(sentences_tst_neg), worddict)\n",
    "X_test_full = test_x_pos + test_x_neg\n",
    "y_test_full = [1] * len(test_x_pos) + [0] * len(test_x_neg)\n",
    "\n",
    "print(X_test_full[0])\n",
    "print(y_test_full[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure sequences for a RNN model\n",
    "    - Remove words with low frequency\n",
    "    - Truncate / complete sequences to the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Median length of sentences\n",
    "print('Median length: ', np.median([len(x) for x in X_test_full]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 50000 # Number of most frequent words selected. the less frequent recode to 0\n",
    "maxlen = 200  # cut texts after this number of words (among top max_features most common words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the most frequent max_features, recode others using 0\n",
    "def remove_features(x):\n",
    "    return [[0 if w >= max_features else w for w in sen] for sen in x]\n",
    "\n",
    "X_train = remove_features(X_train_full)\n",
    "X_test  = remove_features(X_test_full)\n",
    "y_train = y_train_full\n",
    "y_test = y_test_full\n",
    "\n",
    "print(X_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.keras import preprocessing\n",
    "\n",
    "# Cut or complete the sentences to length = maxlen\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "\n",
    "X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export train and test data\n",
    "np.save(data_path + 'X_train', X_train)\n",
    "np.save(data_path + 'y_train', y_train)\n",
    "np.save(data_path + 'X_test',  X_test)\n",
    "np.save(data_path + 'y_test',  y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export worddict\n",
    "import pickle\n",
    "\n",
    "with open(data_path + 'worddict.pickle', 'wb') as pfile:\n",
    "    pickle.dump(worddict, pfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tm]",
   "language": "python",
   "name": "conda-env-tm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
